{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T03:51:36.293013Z",
     "start_time": "2019-11-07T03:51:35.517363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condensed_2009.json\n",
      "condensed_2010.json\n"
     ]
    }
   ],
   "source": [
    "!ls data/trump_tweet_data_archive/unzipped_condensed_json/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading assets/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T03:51:57.206692Z",
     "start_time": "2019-11-07T03:51:56.387088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Concatenate, Embedding, LSTM, Dense, Dropout, BatchNormalization, Input, TimeDistributed, Dot, RepeatVector, Activation, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T03:52:01.885038Z",
     "start_time": "2019-11-07T03:52:01.881649Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './data/trump_tweet_data_archive/unzipped_condensed_json/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T03:52:01.946504Z",
     "start_time": "2019-11-07T03:52:01.905961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 198,\n",
       " {'source': 'Twitter Web Client',\n",
       "  'id_str': '6971079756',\n",
       "  'text': 'From Donald Trump: Wishing everyone a wonderful holiday & a happy, healthy, prosperous New Year. Let’s think like champions in 2010!',\n",
       "  'created_at': 'Wed Dec 23 17:38:18 +0000 2009',\n",
       "  'retweet_count': 28,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'favorite_count': 12,\n",
       "  'is_retweet': False},\n",
       " 'From Donald Trump: Wishing everyone a wonderful holiday & a happy, healthy, prosperous New Year. Let’s think like champions in 2010!')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for filename in listdir(data_dir):\n",
    "    with open(data_dir + filename, 'r') as f:\n",
    "        data += json.loads(f.read())\n",
    "        \n",
    "type(data), len(data), data[0], data[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data, creating training data/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pattern = re.compile(r'\\d+$')\n",
    "year_pattern = re.compile(r'[12][90]\\d\\d$')\n",
    "big_num = re.compile('\\d[\\d,]+$')\n",
    "\n",
    "    \n",
    "def apply_filters(x):\n",
    "    if year_pattern.match(x):\n",
    "        return '<year>'\n",
    "    elif num_pattern.match(x):\n",
    "        return '<num>'\n",
    "    elif big_num.match(x):\n",
    "        return '<bignum>'\n",
    "    elif len(x) > 1 and '.' in x:\n",
    "        return '<url>'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "']\\n:_;+{|%-[`@\\t>*,</#\\\\=^~}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def special_filter(exclude = '', base_filters = '!\"#$%&&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    return ''.join(set(base_filters) - set(exclude))    \n",
    "    \n",
    "special_filter('!?.$&()\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'donald', 'trump', ':', 'wishing', 'everyone', 'a', 'wonderful', 'holiday', '&', 'a', 'happy', ',', 'healthy', ',', 'prosperous', 'new', 'year', '.', 'let', \"'s\", 'think', 'like', 'champions', 'in', '<year>', '!'], ['trump', 'international', 'tower', 'in', 'chicago', 'ranked', '6th', 'tallest', 'building', 'in', 'world', 'by', 'council', 'on', 'tall', 'buildings', '&', 'urban', 'habitat', 'http', ':', '<url>'], ['wishing', 'you', 'and', 'yours', 'a', 'very', 'happy', 'and', 'bountiful', 'thanksgiving', '!'], ['donald', 'trump', 'partners', 'with', 'tv1', 'on', 'new', 'reality', 'series', 'entitled', ',', 'omarosa', \"'s\", 'ultimate', 'merger', ':', 'http', ':', '<url>'], ['--', 'work', 'has', 'begun', ',', 'ahead', 'of', 'schedule', ',', 'to', 'build', 'the', 'greatest', 'golf', 'course', 'in', 'history', ':', 'trump', 'international', '–', 'scotland', '.'], ['--', 'from', 'donald', 'trump', ':', '``', 'ivanka', 'and', 'jared', \"'s\", 'wedding', 'was', 'spectacular', ',', 'and', 'they', 'make', 'a', 'beautiful', 'couple', '.', 'i', \"'m\", 'a', 'very', 'proud', 'father', '.', \"''\"], ['hear', 'donald', 'trump', 'discuss', 'big', 'gov', 'spending', ',', 'banks', ',', '&', 'taxes', 'on', 'your', 'world', 'w/neil', 'cavuto', ':', 'http', ':', '<url>'], ['watch', 'video', 'of', 'ivanka', 'trump', 'sharing', 'business', 'advice', 'with', '<num>', 'entrepreneurial', 'women', 'on', 'gma', ':', 'http', ':', '<url>'], ['read', 'what', 'donald', 'trump', 'has', 'to', 'say', 'about', 'daughter', 'ivanka', \"'s\", 'upcoming', 'new', 'book', ',', 'the', 'trump', 'card', ':', 'http', ':', '<url>'], ['``', 'a', 'lot', 'of', 'people', 'have', 'imagination', ',', 'but', 'ca', \"n't\", 'execute', '--', 'you', 'have', 'to', 'execute', 'with', 'the', 'imagination', '.', \"''\", '--', 'donald', '<url>', 'trump', 'http', ':', '<url>'], ['read', 'donald', 'trump', \"'s\", 'top', 'ten', 'tips', 'for', 'success', ':', 'http', ':', '<url>'], ['more', 'hysterical', 'dsrl', 'videos', 'featuring', 'donald', 'trump', 'and', '``', 'double', 'trump', \"''\", 'plus', 'enter', 'golden', 'lick', 'race', 'sweepstakes', ':', 'http', ':', '<url>'], ['donald', 'trump', 'bids', 'to', 'buy', 'the', 'oreo', 'double', 'stuf', 'racing', 'league', '.', 'check', 'it', 'out', ':', 'http', ':', '<url>'], ['reminder', ':', 'the', 'miss', 'universe', 'competition', 'will', 'be', 'live', 'from', 'the', 'bahamas', 'tonight', '9pm', '(', 'est', ')', 'on', 'nbc', ':', 'http', ':', '<url>'], ['watch', 'the', 'miss', 'universe', 'competition', 'live', 'from', 'the', 'bahamas', 'sunday', ',', '8/23', '9pm', '(', 'et', ')', 'on', 'nbc', ':', 'http', ':', '<url>'], ['--', 'watch', 'donald', 'trump', \"'s\", 'recent', 'appearance', 'on', 'the', 'late', 'show', 'with', 'david', 'letterman', ':', 'http', ':', '<url>'], ['ivanka', 'is', 'now', 'on', 'twitter', 'you', 'can', 'follow', 'her', 'ivankatrump', 'have', 'a', 'terrific', 'weekend', '!'], ['``', 'think', '.', 'that', \"'s\", 'the', 'first', 'step', '.', 'use', 'all', 'your', 'power', 'to', 'utilize', 'and', 'develop', 'that', 'capability', \"''\", '--', 'donald', '<url>', 'trump', 'http', ':', '<url>'], ['browse', 'donald', 'trump', \"'s\", 'summer', 'reading', 'list', 'for', 'business', 'success', 'at', 'the', 'trump', 'university', 'blog', ':', 'http', ':', '<url>'], ['check', 'out', 'a', 'list', 'of', 'donald', 'trump', \"'s\", 'books', 'for', 'summer', 'reading', 'at', 'the', 'trump', 'university', 'blog', ':', 'http', ':', '<url>']]\n"
     ]
    }
   ],
   "source": [
    "def filter_function(word):\n",
    "    return word not in special_filter('!?.,$&():\"')\n",
    "\n",
    "tweet_text = map(lambda x : x['text'].replace('’', \"'\"), data)\n",
    "nltk_tokenized_tweets = map(word_tokenize, tweet_text)\n",
    "filtered_tokenized_tweets = map(lambda x : list(filter(filter_function, x)), nltk_tokenized_tweets)\n",
    "lowered_tokenized_tweets = map(lambda x : list(map(str.lower, x)), filtered_tokenized_tweets)\n",
    "placeholder_subbed_tweets = map(lambda x : list(map(apply_filters, x)), lowered_tokenized_tweets)\n",
    "\n",
    "processed_text = list(placeholder_subbed_tweets)\n",
    "\n",
    "print(processed_text[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = Counter()\n",
    "for processed_tweet in processed_text:\n",
    "    for word in processed_tweet:\n",
    "        wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<url> ,  the\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "vocab = list(map(lambda x : x[0], wordcounts.most_common(VOCAB_SIZE)))\n",
    "vocab.append('<end>')\n",
    "vocab.append('<unk>')\n",
    "word_index = {\n",
    "    word : index + 1 for index, word in enumerate(vocab)\n",
    "}\n",
    "index_word = {index : word for word, index in word_index.items()}\n",
    "print(index_word[1], ', ', index_word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36, 19, 8, 4, 106, 68, 7, 195, 261, 37, 7, 58, 5, 262, 5, 263, 29, 196, 3, 389, 13, 69, 59, 197, 17, 96, 18], [8, 60, 97, 17, 264, 390, 391, 392, 393, 17, 78, 70, 394, 9, 395, 396, 37, 397, 398, 10, 4, 1], [106, 40, 11, 399, 7, 79, 58, 11, 400, 198, 18]]\n"
     ]
    }
   ],
   "source": [
    "def word_to_seq(word):\n",
    "    if word in word_index:\n",
    "        return word_index[word]\n",
    "    else:\n",
    "        return word_index['<unk>']\n",
    "    \n",
    "processed_seq = [ [word_to_seq(word) for word in tweet] for tweet in processed_text]\n",
    "print(processed_seq[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], [1], [1, 2], [1, 2, 3], [1, 2, 3, 4]) (1, 2, 3, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "def seq_to_samples(seq, end_token_index):\n",
    "    samples = [\n",
    "        (seq[:i], seq[i])\n",
    "        for i in range(len(seq))\n",
    "    ]\n",
    "    samples.append((seq, end_token_index))\n",
    "    return list(zip(*samples))\n",
    "\n",
    "X, Y = seq_to_samples([1,2,3,4], 5)\n",
    "print(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4711, 4711)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "for seq in processed_seq:\n",
    "    s, l = seq_to_samples(seq, word_index['<end>'])\n",
    "    samples.extend(s)\n",
    "    labels.extend(l)\n",
    "    \n",
    "len(samples), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]  36 from\n",
      "[36] from 19 donald\n",
      "[36, 19] from donald 8 trump\n",
      "[36, 19, 8] from donald trump 4 :\n",
      "[36, 19, 8, 4] from donald trump : 106 wishing\n",
      "[36, 19, 8, 4, 106] from donald trump : wishing 68 everyone\n",
      "[36, 19, 8, 4, 106, 68] from donald trump : wishing everyone 7 a\n",
      "[36, 19, 8, 4, 106, 68, 7] from donald trump : wishing everyone a 195 wonderful\n",
      "[36, 19, 8, 4, 106, 68, 7, 195] from donald trump : wishing everyone a wonderful 261 holiday\n",
      "[36, 19, 8, 4, 106, 68, 7, 195, 261] from donald trump : wishing everyone a wonderful holiday 37 &\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(samples[i], ' '.join([index_word[word] for word in samples[i]]), labels[i], index_word[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  36],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  36,  19],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  36,  19,   8],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,  36,  19,   8,   4],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,  36,  19,   8,   4, 106],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,  36,  19,   8,   4, 106,  68],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,  36,  19,   8,   4, 106,  68,   7],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,  36,  19,   8,   4, 106,  68,   7, 195],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,  36,  19,   8,   4, 106,  68,   7, 195, 261]]),\n",
       " (4711, 39))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([len(seq) for seq in processed_seq])\n",
    "max_seq_len, len(processed_seq)\n",
    "\n",
    "X = pad_sequences(samples, maxlen= max_seq_len, padding = 'pre')\n",
    "X[0:10], X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4711, 972)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = max(labels) + 1\n",
    "Y = to_categorical(labels, num_classes= num_classes)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4711, 39), (4711, 972))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 39, 300)           291600    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 39, 128)           219648    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 39, 128)           512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 972)               125388    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 972)               3888      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 972)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 972)               945756    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 972)               3888      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 972)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 972)               945756    \n",
      "=================================================================\n",
      "Total params: 2,668,532\n",
      "Trainable params: 2,664,132\n",
      "Non-trainable params: 4,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_drop = 0.2\n",
    "dense_drop = 0.2\n",
    "embed_dim = 300\n",
    "lstm_dims = 128\n",
    "dense_dim = num_classes\n",
    "\n",
    "deeptrump = tf.keras.Sequential([\n",
    "    Input(shape = (X.shape[1])),\n",
    "    Embedding(num_classes, embed_dim, mask_zero = True),\n",
    "    LSTM(lstm_dims, return_sequences = True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(lstm_drop),\n",
    "    LSTM(lstm_dims),\n",
    "    BatchNormalization(),\n",
    "    Dropout(lstm_drop),\n",
    "    Dense(dense_dim, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dense_drop),\n",
    "    Dense(dense_dim, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dense_drop),\n",
    "    Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(0.001)\n",
    "deeptrump.compile(loss= 'categorical_crossentropy', optimizer = optimizer, metrics = ['acc'])\n",
    "deeptrump.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\fit\\20191111-172616\n",
      "Train on 4711 samples\n",
      "Epoch 1/2\n",
      "  32/4711 [..............................] - ETA: 18s - loss: 2.4905 - acc: 0.5000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.276255). Check your callbacks.\n",
      "4711/4711 [==============================] - 19s 4ms/sample - loss: 3.1309 - acc: 0.3197\n",
      "Epoch 2/2\n",
      "4711/4711 [==============================] - 17s 4ms/sample - loss: 2.7326 - acc: 0.3808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cb05a6f0b8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir, histogram_freq=1)\n",
    "print(logdir)\n",
    "deeptrump.fit(X,Y, epochs = 2, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 39, 300)           291600    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 39, 256)           439296    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 39, 256)           1024      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 39, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 972)               249804    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 972)               3888      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 972)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 972)               945756    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 972)               3888      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 972)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 972)               945756    \n",
      "=================================================================\n",
      "Total params: 3,407,348\n",
      "Trainable params: 3,402,436\n",
      "Non-trainable params: 4,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_drop = 0.5\n",
    "dense_drop = 0.5\n",
    "embed_dim = 300\n",
    "lstm_dims = 128\n",
    "dense_dim = num_classes\n",
    "\n",
    "deep_bi_trump = tf.keras.Sequential([\n",
    "    Input(shape = (X.shape[1],)),\n",
    "    Embedding(num_classes, embed_dim, mask_zero = True),\n",
    "    tf.keras.layers.Bidirectional(LSTM(lstm_dims, return_sequences = True)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(lstm_drop),\n",
    "    LSTM(2*lstm_dims),\n",
    "    BatchNormalization(),\n",
    "    Dropout(lstm_drop),\n",
    "    Dense(dense_dim, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dense_drop),\n",
    "    Dense(dense_dim, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(dense_drop),\n",
    "    Dense(num_classes, activation = 'softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(0.001)\n",
    "deep_bi_trump.compile(loss= 'categorical_crossentropy', optimizer = optimizer, metrics = ['acc'])\n",
    "deep_bi_trump.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmax(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, z, mask = None):\n",
    "        \n",
    "        z = tf.keras.backend.exp(z)\n",
    "        \n",
    "        if not mask is None:\n",
    "            assert(mask.shape == z.shape), 'Mask has incorrect dimensions: ' + str(z.shape) + ' vs. ' + str(mask.shape)\n",
    "            z = tf.multiply(z, tf.dtypes.cast(mask, 'float32'))\n",
    "        \n",
    "        return tf.divide(z, tf.reduce_sum(z, axis = -1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, dense_units, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        assert(type(dense_units) in [tuple, list]), 'Argument dense units must be an iterable (10,5,2) etc.'\n",
    "        self.dense_units = dense_units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_densor = Dense(self.dense_units[0], activation = 'tanh')\n",
    "        if len(input_shape) > 1:\n",
    "            self.densors = [Dense(nodes, activation = 'tanh') for nodes in self.dense_units[1:]]\n",
    "        else:\n",
    "            self.densors = []\n",
    "        self.output_densor = Dense(1, activation = 'relu')\n",
    "        self.dot = Dot(axes = 1)\n",
    "        self.activator = MaskedSoftmax()\n",
    "        self.repeater = RepeatVector(input_shape[1])\n",
    "        self.concatenator = Concatenate(axis = -1)\n",
    "    \n",
    "    def call(self, a, s_prev = None, mask = None):\n",
    "                 \n",
    "        if not s_prev is None:\n",
    "            \n",
    "            assert(s_prev.shape[-1] == a.shape[-1]), 's_prev must have same last dimension as a_prev.'\n",
    "            \n",
    "            alpha = self.concatenator([a, self.repeater(s_prev)])\n",
    "            \n",
    "        else:\n",
    "            alpha = a\n",
    "        \n",
    "        alpha = self.input_densor(alpha)\n",
    "        \n",
    "        for densor in self.densors:\n",
    "            alpha = densor(alpha)\n",
    "        \n",
    "        alpha = self.output_densor(alpha)\n",
    "        \n",
    "        alpha = tf.squeeze(alpha, axis = -1)\n",
    "        \n",
    "        alpha = self.activator(alpha, mask = mask)\n",
    "        \n",
    "        print(alpha, a)\n",
    "            \n",
    "        context = self.dot([alpha, a])\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(10, 5, mask_zero=True)\n",
    "rnn = Bidirectional(LSTM(3, return_sequences=True))\n",
    "attn = Attention((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.         0.         0.         0.5002208  0.4997792 ]\n",
      " [0.33333334 0.         0.         0.33333334 0.33333334]], shape=(2, 5), dtype=float32) tf.Tensor(\n",
      "[[[ 0.          0.          0.         -0.00092819  0.00463552\n",
      "   -0.0011662 ]\n",
      "  [ 0.          0.          0.         -0.00092819  0.00463552\n",
      "   -0.0011662 ]\n",
      "  [ 0.          0.          0.         -0.00092819  0.00463552\n",
      "   -0.0011662 ]\n",
      "  [-0.0058745   0.00543387 -0.00510851 -0.00092819  0.00463552\n",
      "   -0.0011662 ]\n",
      "  [-0.00405702 -0.0014659  -0.00272797  0.00436322 -0.00468217\n",
      "   -0.00298495]]\n",
      "\n",
      " [[ 0.0016803   0.003492   -0.00670708 -0.00411446 -0.01846154\n",
      "   -0.00603914]\n",
      "  [ 0.0016803   0.003492   -0.00670708  0.00380335 -0.02956325\n",
      "   -0.00619356]\n",
      "  [ 0.0016803   0.003492   -0.00670708  0.00380335 -0.02956325\n",
      "   -0.00619356]\n",
      "  [ 0.01034939 -0.00875904  0.00422394  0.00380335 -0.02956325\n",
      "   -0.00619356]\n",
      "  [ 0.01443224 -0.01694424  0.01321637  0.00337907 -0.01684443\n",
      "   -0.00367955]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=33159, shape=(2, 6), dtype=float32, numpy=\n",
       "array([[-4.9661640e-03,  1.9855106e-03, -3.9187628e-03,  1.7163439e-03,\n",
       "        -2.1265354e-05, -2.0751706e-03],\n",
       "       [ 8.8206455e-03, -7.4037602e-03,  3.5777446e-03,  1.0226513e-03,\n",
       "        -2.1623071e-02, -5.3040832e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0,0,5,8],[1,0,0,3,3]])\n",
    "\n",
    "\n",
    "X = embed(X)\n",
    "X = rnn(X)\n",
    "X = attn(X, np.zeros((2,6)))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
